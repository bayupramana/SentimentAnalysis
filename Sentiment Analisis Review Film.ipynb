{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0204b32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bayu\n",
      "[nltk_data]     Pramana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Bayu Pramana\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# import nltk library untuk tokenizing, lemmatization dan remove stopword\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f8854",
   "metadata": {},
   "source": [
    "# Load dataset review film berbahasa Inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087b35a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = pd.read_csv(\"data/dataset_movie_review.csv\")\n",
    "review_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc0ea5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1815aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bayu Pramana\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tampilkan jumlah sentiment dengan grafik\n",
    "sns.countplot(review_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438f6edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  negative\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ab01e",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081ac404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merubah seluruh review menjadi lowercase\n",
    "review_data['review'] = review_data['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406b8eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive\n",
       "5  probably my all-time favorite movie, a story o...  positive\n",
       "6  i sure would like to see a resurrection of a u...  positive\n",
       "7  this show was an amazing, fresh & innovative i...  negative\n",
       "8  encouraged by the positive comments about this...  negative\n",
       "9  if you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menghapus seluruh digit angka yang ada pada data review\n",
    "def removeDigit(sentence):\n",
    "    result = re.sub(r\"\\d\", \"\", sentence)\n",
    "    return result\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(removeDigit)\n",
    "review_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e128f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive\n",
       "5  probably my all-time favorite movie, a story o...  positive\n",
       "6  i sure would like to see a resurrection of a u...  positive\n",
       "7  this show was an amazing, fresh & innovative i...  negative\n",
       "8  encouraged by the positive comments about this...  negative\n",
       "9  if you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menghapus seluruh tag HTML yang ada pada data review\n",
    "def removeTag_html(sentence):\n",
    "    result = re.sub(r'<.*?>','',sentence)\n",
    "    return result\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(removeTag_html)\n",
    "review_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1637a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus seluruh spesial character seperti ,./.()\n",
    "def removingSpecialChar(sentence):\n",
    "    result = re.sub(r'[^\\w]', ' ', sentence)\n",
    "    return result\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(removingSpecialChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e609c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus spasi berlebih\n",
    "def removingMultipleSpacing(sentence):\n",
    "    result = re.sub('\\s+',' ',sentence)\n",
    "    return result\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(removingMultipleSpacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc47b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpan data hasil preprocessing\n",
    "review_data.to_csv('data/preprocessing/clean_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a75791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus seluruh stopword yang ada dalam review data menggunakan NLTK\n",
    "def removingStopwords(sentence):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    text_tokens = word_tokenize(sentence)\n",
    "    filtered_word = [word for word in text_tokens if word not in stopword_list]\n",
    "    filtered_sentence = (\" \").join(filtered_word)\n",
    "    return filtered_sentence\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(removingStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42be521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpan data hasil penghapusan stopword sebagai data tanpa lemmatization\n",
    "review_data.to_csv('data/preprocessing/dataset_without_lemmatization.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db833c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendapatkan postag tiap kata yang ada di dalam data review\n",
    "def get_postag_word(word):\n",
    "    postag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    postag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return postag_dict.get(postag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a39d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization data review berdasarkan postag menggunakan WordNetLemmatizer\n",
    "def lemmatization_word(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence= ' '.join([lemmatizer.lemmatize(word, get_postag_word(word)) for word in sentence.split()])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "review_data['review'] = review_data['review'].apply(lemmatization_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f158281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpan data hasil lemmatization\n",
    "review_data.to_csv('data/preprocessing/dataset_using_lemmatization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e96703",
   "metadata": {},
   "source": [
    "# Extraksi menggunakan word2vec dengan data lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1e8ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mention watch oz episode hooked r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stun fi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure would like see resurrection date seahunt ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show amaze fresh innovative idea first air fir...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encourage positive comment film look forward w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like original gut wrench laughter like movie y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewer mention watch oz episode hooked r...  positive\n",
       "1  wonderful little production film technique una...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically family little boy jake think zombie ...  negative\n",
       "4  petter mattei love time money visually stun fi...  positive\n",
       "5  probably time favorite movie story selflessnes...  positive\n",
       "6  sure would like see resurrection date seahunt ...  positive\n",
       "7  show amaze fresh innovative idea first air fir...  negative\n",
       "8  encourage positive comment film look forward w...  negative\n",
       "9  like original gut wrench laughter like movie y...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset hasil lemmatization\n",
    "review_data = pd.read_csv('data/preprocessing/dataset_using_lemmatization.csv')\n",
    "review_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79893a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah review positif = 1 dan negatif = 0\n",
    "review_data['sentiment'] = review_data['sentiment'].map({'positive':1, 'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99568764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisahkan data menjadi corpus data untuk word2vec dan untuk train test model\n",
    "corpus_data = review_data.iloc[:45000] # corpus data word2vec sebesar 45000 data\n",
    "train_test = review_data.iloc[45000:] # train test model sebesar 5000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a05a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data.to_csv('data/preprocessing/corpus_lemmatization.csv')\n",
    "\n",
    "train_test.to_csv('data/preprocessing/data_model_lemmatization.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eecb60",
   "metadata": {},
   "source": [
    "## Word2vec dengan 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "275ef0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# tokenisasi seluruh kata yang ada dalam corpus yang menggunakan lemmatization\n",
    "sentence_corpus = [word_tokenize(review) for review in corpus_data.review] \n",
    "\n",
    "# bangun model skip-gram dengan vektor size 100 dan window 5\n",
    "model_100 = Word2Vec(\n",
    "            sentence_corpus,\n",
    "            vector_size = 100, # vektor size yang digunakan = 100\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "\n",
    "#simpan model yang telah bangun\n",
    "model_100.save(\"model/lemmatization/word2vec_100.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca6c88d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15749273,  0.45180756, -0.10939796,  0.37131956,  0.03510791,\n",
       "       -0.10384069, -0.1046925 ,  0.48323312, -0.37503   , -0.1931874 ,\n",
       "       -0.10173449, -0.2238978 ,  0.05934507, -0.05434404, -0.04043657,\n",
       "       -0.16520752,  0.07978155, -0.10826575, -0.24566951, -0.66446126,\n",
       "        0.5150075 , -0.04622632,  0.08739759,  0.07124893, -0.08877259,\n",
       "       -0.03109653, -0.4196705 , -0.03160298,  0.00539299,  0.2560579 ,\n",
       "        0.24828333,  0.0822815 ,  0.23566908, -0.73987734, -0.01224797,\n",
       "        0.08982772,  0.4041815 , -0.00530707, -0.09679309, -0.28624424,\n",
       "       -0.01964009, -0.10254408, -0.36908364, -0.11943126, -0.10628622,\n",
       "       -0.09394993, -0.31710324, -0.08454562, -0.15658006,  0.23581766,\n",
       "        0.18248108, -0.36479092, -0.27410766,  0.04809016,  0.01019737,\n",
       "        0.07517741,  0.2995484 ,  0.01625962, -0.2643239 ,  0.15770556,\n",
       "       -0.15836471, -0.04668534,  0.1363566 ,  0.11172778, -0.16109684,\n",
       "        0.2231512 ,  0.23120101,  0.2223919 , -0.3050328 ,  0.25025496,\n",
       "       -0.23968703,  0.2879987 ,  0.19336511, -0.48139247,  0.23051469,\n",
       "        0.21329582,  0.07698341, -0.21977937, -0.3126135 ,  0.05898011,\n",
       "       -0.2605118 , -0.16930316, -0.1352449 ,  0.27055228, -0.1689753 ,\n",
       "       -0.03294208,  0.15029374, -0.01333188,  0.22212827,  0.26338828,\n",
       "        0.06247405,  0.13569118, -0.05815879, -0.22057384,  0.5544653 ,\n",
       "        0.2766464 ,  0.21831965, -0.07517573, -0.10464392,  0.06431237],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "model_100.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7681b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model yang berisi word vector dari model yang dibangun\n",
    "model_100 = Word2Vec.load(\"model/lemmatization/word2vec_100.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77650db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function untuk memvektorisasi seluruh kalimat review dalam dataset\n",
    "def sent_vector(sentence, model):\n",
    "    vecs = [model[word] for word in word_tokenize(sentence) if word in model]\n",
    "    sent_vec = np.mean(vecs, axis=0)\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b213ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "vecs = [sent_vector(sentence, model_100) for sentence in train_test.review]\n",
    "features_vect = np.array(vecs)\n",
    "data_features = pd.DataFrame(features_vect)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "data_features.to_csv('data/features/lemmatization_features_100d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bfeae",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b0fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  78.9\n",
      "Recall score =  78.89\n",
      "Precission score =  78.91\n",
      "F1-Score =  78.86\n",
      "Wall time: 410 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "acc_score = cross_val_score(gnb_model, data_features, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "avg_accuracy = round((acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "recall_score = cross_val_score(gnb_model, data_features, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "avg_recall = round((recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "precission_score = cross_val_score(gnb_model, data_features, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "avg_precission = round((precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "f_score = cross_val_score(gnb_model, data_features, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "avg_fscore = round((f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", avg_accuracy*100)\n",
    "print(\"Recall score = \", avg_recall*100)\n",
    "print(\"Precission score = \", avg_precission*100)\n",
    "print(\"F1-Score = \", avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3f6f5",
   "metadata": {},
   "source": [
    "## Word2Vec dengan 200 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4a1a0821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 200 dan window 5\n",
    "model_200 = Word2Vec(\n",
    "            sentence_corpus,\n",
    "            vector_size = 200, # vektor size yang digunakan = 200\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "# simpan model yang telah dibangun\n",
    "model_200.save(\"model/lemmatization/word2vec_200.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3efa1378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00582115,  0.09987962,  0.22737178,  0.27336118,  0.17503077,\n",
       "       -0.34968898,  0.17272046,  0.4445207 , -0.05265582,  0.04252632,\n",
       "       -0.06917235, -0.19861403,  0.08129756,  0.34716538, -0.03370559,\n",
       "       -0.0432951 , -0.19194989, -0.00677023, -0.10312586, -0.47811392,\n",
       "        0.2210971 , -0.04217154,  0.19409648, -0.0305879 ,  0.10523665,\n",
       "        0.02704805,  0.05499478,  0.02880051, -0.36526254, -0.07991792,\n",
       "        0.07095156,  0.24160664,  0.21380606, -0.0732311 , -0.1713054 ,\n",
       "       -0.03036325, -0.1000858 , -0.2425635 , -0.0863069 , -0.00707094,\n",
       "       -0.08487222,  0.01331556, -0.10096612,  0.11725198,  0.07547568,\n",
       "       -0.04288567, -0.0586387 , -0.08240765,  0.24401765,  0.13014126,\n",
       "        0.02870303,  0.05262419, -0.08478595, -0.08759098,  0.17011365,\n",
       "       -0.01621709,  0.04883318, -0.22335799, -0.14714597, -0.02561124,\n",
       "       -0.03139586,  0.05656885,  0.02624772, -0.13656689, -0.22482094,\n",
       "        0.25078195,  0.06127713,  0.49240044, -0.3468184 ,  0.32883582,\n",
       "        0.13611898, -0.01942348,  0.18651126, -0.121055  ,  0.04926508,\n",
       "       -0.14170006,  0.10992278, -0.09614853, -0.15104483, -0.03954606,\n",
       "       -0.16936268, -0.01292151, -0.0724192 ,  0.29754695, -0.00579756,\n",
       "       -0.19306011,  0.09197576,  0.18737729,  0.06726129,  0.02282983,\n",
       "        0.3987619 ,  0.09366563,  0.18952607,  0.17675035,  0.06491658,\n",
       "        0.2374231 ,  0.02323175, -0.04479381,  0.03934707,  0.02175068,\n",
       "       -0.12896922,  0.2828633 ,  0.14396207, -0.02840274,  0.08219247,\n",
       "       -0.20782764, -0.00303022,  0.27197722, -0.25855568, -0.16704077,\n",
       "       -0.1763874 , -0.24024664, -0.01748865,  0.08167927,  0.16533893,\n",
       "       -0.03963107,  0.19179614, -0.17452116, -0.11076391,  0.00231238,\n",
       "        0.15393852, -0.18175961,  0.25598413, -0.15533729, -0.08030359,\n",
       "       -0.08008447,  0.03786353,  0.06695055, -0.03249414,  0.09711964,\n",
       "        0.2841006 , -0.31428865, -0.01952481, -0.43096158, -0.01803421,\n",
       "        0.1466091 ,  0.19933873,  0.06820855, -0.12234216, -0.39142781,\n",
       "        0.09965162, -0.11837444, -0.2434746 ,  0.03480713, -0.12787913,\n",
       "       -0.05598028, -0.10722639,  0.02587314,  0.02193747, -0.13004461,\n",
       "        0.20985332, -0.3065585 , -0.0079922 , -0.05515323, -0.20682876,\n",
       "        0.12150346,  0.06717777,  0.37871525, -0.09716524,  0.1210568 ,\n",
       "       -0.03057153,  0.02962859, -0.06308775, -0.0478348 ,  0.05227914,\n",
       "        0.24148354,  0.24847877, -0.2519863 ,  0.04110008,  0.19921075,\n",
       "       -0.12716784, -0.01906472,  0.14398876, -0.10201175,  0.21038023,\n",
       "       -0.07228919, -0.498015  ,  0.12028623, -0.16745546,  0.00326709,\n",
       "       -0.14791186, -0.08082125,  0.09685457, -0.02902978,  0.19919036,\n",
       "        0.00458613,  0.10551532,  0.1377833 ,  0.3830815 , -0.17204078,\n",
       "        0.07407714,  0.1748507 , -0.00457533, -0.16797435, -0.00737921,\n",
       "       -0.00981822, -0.04901324, -0.1323636 , -0.19841331, -0.21906212],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "model_200.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955839cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model yang berisi word vector dari model yang dibangun\n",
    "model_200 = Word2Vec.load(\"model/lemmatization/word2vec_200.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95eb7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "vecs200 = [sent_vector(sentence, model_200) for sentence in train_test.review]\n",
    "features_vect200 = np.array(vecs200)\n",
    "data_features200 = pd.DataFrame(features_vect200)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "data_features200.to_csv('data/features/lemmatization_features_200d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a45d8",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc85a9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  79.24\n",
      "Recall score =  79.22\n",
      "Precission score =  79.25999999999999\n",
      "F1-Score =  79.19\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "acc_score = cross_val_score(gnb_model, data_features200, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "avg_accuracy = round((acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "recall_score = cross_val_score(gnb_model, data_features200, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "avg_recall = round((recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "precission_score = cross_val_score(gnb_model, data_features200, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "avg_precission = round((precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "f_score = cross_val_score(gnb_model, data_features200, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "avg_fscore = round((f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", avg_accuracy*100)\n",
    "print(\"Recall score = \", avg_recall*100)\n",
    "print(\"Precission score = \", avg_precission*100)\n",
    "print(\"F1-Score = \", avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee413f57",
   "metadata": {},
   "source": [
    "## Word2Vec dengan 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "237aae1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 300 dan window 5\n",
    "model_300 = Word2Vec(\n",
    "            sentence_corpus,\n",
    "            vector_size = 300,# vektor size yang digunakan = 300\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "# simpan model yang telah dibangun\n",
    "model_300.save(\"model/lemmatization/word2vec_300.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3e393177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.11846426e-02,  9.66117382e-02, -1.65604845e-01,  1.12960525e-01,\n",
       "       -2.83073515e-01, -4.39061403e-01, -2.70224046e-02,  2.15059817e-01,\n",
       "        3.44586000e-02, -6.22466914e-02,  1.32076234e-01, -9.64351445e-02,\n",
       "       -1.65261522e-01,  1.00265622e-01, -2.99863160e-01, -2.08668381e-01,\n",
       "        1.14369705e-01, -4.16322239e-02,  1.36474848e-01, -2.09378958e-01,\n",
       "        2.53736913e-01,  1.07871860e-01,  1.04405627e-01,  1.07033856e-01,\n",
       "        7.16485903e-02, -5.77353500e-02, -1.29103661e-01,  1.83933079e-01,\n",
       "       -1.50836885e-01, -6.79277256e-02, -9.28752795e-02, -1.33521929e-02,\n",
       "        4.84470204e-02,  1.51157245e-01, -1.00246251e-01,  8.73218104e-03,\n",
       "        1.31278649e-01, -1.89960107e-01,  1.16867190e-02, -5.03356159e-02,\n",
       "       -3.06063086e-01,  2.23445669e-02, -1.85213625e-01, -2.24435344e-01,\n",
       "        2.32117027e-01,  2.52899647e-01, -3.43964389e-03,  1.06025681e-01,\n",
       "       -1.48634603e-02,  9.53945890e-02, -1.77879520e-02,  1.12974048e-01,\n",
       "       -6.57053292e-02, -6.48993393e-03,  1.13081545e-01, -6.85484037e-02,\n",
       "        2.07291171e-01, -1.26511306e-01, -4.65472937e-02, -3.27064991e-02,\n",
       "        5.84152341e-03, -2.68861353e-01, -1.73148528e-01, -1.03598565e-01,\n",
       "       -6.24024197e-02,  7.65022635e-02, -1.19911451e-02,  1.03545666e-01,\n",
       "        6.74566552e-02,  7.14249387e-02, -6.15579030e-03,  9.52974185e-02,\n",
       "        4.66988683e-02,  4.21848599e-05,  2.11033095e-02,  8.85234997e-02,\n",
       "       -2.05508962e-01, -2.12424453e-02, -8.97713080e-02, -1.34704933e-02,\n",
       "       -1.51433885e-01, -4.94694412e-02,  1.90960634e-02,  3.90728205e-01,\n",
       "       -5.73381595e-03,  4.00451273e-02,  1.26751482e-01,  4.32123654e-02,\n",
       "        5.09198606e-02,  3.88523154e-02,  2.83213444e-02, -1.17467910e-01,\n",
       "        3.89023535e-02, -4.81021032e-02,  2.67311446e-02,  3.10323060e-01,\n",
       "        1.53097242e-01,  9.04025659e-02,  2.34056264e-02,  1.15317784e-01,\n",
       "       -1.07040673e-01, -2.63873618e-02,  7.76355788e-02, -1.39286211e-02,\n",
       "       -5.80382384e-02, -1.58786222e-01, -6.24373816e-02,  1.16806403e-02,\n",
       "       -2.78609216e-01,  7.39861280e-02, -3.10975105e-01,  3.79970856e-02,\n",
       "       -2.05937967e-01, -1.26388669e-01,  1.31208062e-01, -5.41208982e-02,\n",
       "       -2.76103639e-03,  3.34185436e-02,  8.49077925e-02, -6.32601678e-02,\n",
       "        1.34714857e-01, -6.04449064e-02,  1.41449273e-01,  1.06306180e-01,\n",
       "       -2.04517633e-01,  1.21878460e-01,  1.27013892e-01, -7.91968256e-02,\n",
       "       -6.99749961e-02,  1.82945654e-01,  2.19371468e-02,  2.06898734e-01,\n",
       "       -5.58620356e-02, -3.65080535e-01,  5.99925464e-04,  5.16416617e-02,\n",
       "       -2.40047246e-01, -3.21101211e-02,  5.33041060e-02,  3.02409823e-03,\n",
       "        1.97136045e-01, -3.24757814e-01, -9.66226310e-02,  1.56811118e-01,\n",
       "        2.00328916e-01, -7.81458095e-02, -3.23525608e-01, -1.17528118e-01,\n",
       "        1.78926989e-01,  1.27318770e-01,  2.94188745e-02, -3.10455561e-01,\n",
       "       -4.03522737e-02, -5.32145202e-02,  1.44091740e-01,  2.08736181e-01,\n",
       "       -1.64683998e-01, -1.90276474e-01, -2.32025329e-02, -6.45877495e-02,\n",
       "        8.99453089e-03,  1.00462481e-01, -1.08732849e-01,  1.07850932e-01,\n",
       "       -1.04452170e-01,  1.94847196e-01, -1.28371209e-01,  3.47725451e-02,\n",
       "        2.52786521e-02,  2.01756895e-01, -8.60925838e-02, -9.86758620e-02,\n",
       "        1.67227030e-01, -3.44147459e-02,  1.54887468e-01,  6.07582293e-02,\n",
       "        8.01887177e-03, -1.20517306e-01,  1.24932215e-01,  4.21452112e-02,\n",
       "       -2.33810842e-01,  8.10031518e-02, -6.75968155e-02, -8.55933204e-02,\n",
       "       -1.02973990e-01, -6.58851117e-02,  4.41069119e-02,  2.18936086e-01,\n",
       "       -9.99571085e-02, -1.71156928e-01,  2.33996354e-04,  1.62599832e-01,\n",
       "       -2.41389617e-01, -6.63560703e-02,  1.60963088e-01,  5.60632758e-02,\n",
       "        3.80297117e-02, -1.74809203e-01,  2.72998605e-02,  7.88216218e-02,\n",
       "       -3.17115188e-01,  1.38988286e-01, -2.96508707e-02, -2.58836687e-01,\n",
       "        1.12629436e-01, -1.86317772e-01, -4.09209020e-02,  4.09875847e-02,\n",
       "       -1.09343246e-01,  7.84411654e-02, -1.52094867e-02,  1.68905985e-02,\n",
       "        1.33651108e-01, -1.36243403e-01,  1.48117682e-02, -1.43467158e-01,\n",
       "        1.30961806e-01, -1.91192105e-01, -3.19781512e-01, -1.52956560e-01,\n",
       "       -4.91675623e-02,  1.60509348e-02, -1.72849834e-01, -1.69688717e-01,\n",
       "        1.25189483e-01, -7.38209039e-02,  2.67109632e-01, -4.16598991e-02,\n",
       "       -2.21637711e-01,  4.21855710e-02,  1.92213103e-01, -5.03014214e-02,\n",
       "       -1.52548268e-01, -4.29258980e-02, -2.36090809e-01,  1.83893740e-02,\n",
       "        1.36535555e-01,  2.40891561e-01,  1.76472977e-01, -2.58621633e-01,\n",
       "       -7.46205300e-02, -1.69362292e-01, -1.08038992e-01,  6.14992082e-02,\n",
       "        1.26285061e-01, -1.42964572e-01, -1.55514684e-02, -3.14101130e-02,\n",
       "        1.25337884e-01,  9.32247043e-02,  6.77117929e-02,  1.60343554e-02,\n",
       "       -1.07281366e-02, -3.14925518e-03,  4.06293198e-02, -5.04647754e-02,\n",
       "        2.64240921e-01, -8.17959160e-02, -3.87024522e-01, -2.55128860e-01,\n",
       "        1.02069139e-01,  4.80281897e-02,  1.61257520e-01, -2.78037846e-01,\n",
       "       -1.38376489e-01,  2.28124335e-01,  6.20126352e-02,  1.69739321e-01,\n",
       "       -2.41892502e-01, -1.19292974e-01, -6.26176894e-02,  1.36212108e-03,\n",
       "       -2.79844906e-02,  1.88480392e-01,  5.23791276e-02,  2.22229943e-01,\n",
       "        2.27932200e-01, -8.89946893e-02, -9.83786508e-02, -1.27185091e-01,\n",
       "        4.33023535e-02,  7.66874477e-02, -1.32689789e-01,  1.89200178e-01,\n",
       "       -4.83311489e-02,  2.81310664e-03, -1.57228038e-01,  1.43498853e-01,\n",
       "        5.36184236e-02,  1.47859484e-01,  5.56028485e-02,  2.13788986e-01,\n",
       "        1.98084548e-01, -1.61400348e-01, -1.44423572e-02,  2.93276370e-01,\n",
       "       -7.91015625e-02, -2.45376587e-01,  3.35234493e-01, -2.94488072e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "model_300.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fa652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model yang berisi word vector dari model yang dibangun\n",
    "model_300 = Word2Vec.load(\"model/lemmatization/word2vec_300.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2f0d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "vecs300 = [sent_vector(sentence, model_300) for sentence in train_test.review]\n",
    "features_vect300 = np.array(vecs300)\n",
    "data_features300 = pd.DataFrame(features_vect300)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "data_features300.to_csv('data/features/lemmatization_features_300d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65c5d4",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1f540de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  79.46\n",
      "Recall score =  79.45\n",
      "Precission score =  79.49000000000001\n",
      "F1-Score =  79.41\n",
      "Wall time: 806 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "acc_score = cross_val_score(gnb_model, data_features300, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "avg_accuracy = round((acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "recall_score = cross_val_score(gnb_model, data_features300, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "avg_recall = round((recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "precission_score = cross_val_score(gnb_model, data_features300, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "avg_precission = round((precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "f_score = cross_val_score(gnb_model, data_features300, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "avg_fscore = round((f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", avg_accuracy*100)\n",
    "print(\"Recall score = \", avg_recall*100)\n",
    "print(\"Precission score = \", avg_precission*100)\n",
    "print(\"F1-Score = \", avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b19286",
   "metadata": {},
   "source": [
    "# Extraksi menggunakan word2vec dengan data tanpa lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adedc410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching oz episode ho...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show amazing fresh innovative idea first aired...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged positive comments film looking forw...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like original gut wrenching laughter like movi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mentioned watching oz episode ho...  positive\n",
       "1  wonderful little production filming technique ...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically family little boy jake thinks zombie...  negative\n",
       "4  petter mattei love time money visually stunnin...  positive\n",
       "5  probably time favorite movie story selflessnes...  positive\n",
       "6  sure would like see resurrection dated seahunt...  positive\n",
       "7  show amazing fresh innovative idea first aired...  negative\n",
       "8  encouraged positive comments film looking forw...  negative\n",
       "9  like original gut wrenching laughter like movi...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset tanpa lemmatization\n",
    "no_lemma_review = pd.read_csv('data/preprocessing/dataset_without_lemmatization.csv')\n",
    "no_lemma_review.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7927c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah review positif = 1 dan negatif = 0\n",
    "no_lemma_review['sentiment'] = no_lemma_review['sentiment'].map({'positive':1, 'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fe162b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for corpus word2vec, train and test\n",
    "no_lemma_corpus_data = no_lemma_review.iloc[:45000] # corpus data word2vec sebesar 45000 data\n",
    "no_lemma_train_test = no_lemma_review.iloc[45000:] # train test model sebesar 5000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880bb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lemma_corpus_data.to_csv('data/preprocessing/no_lemmatization_corpus_data.csv')\n",
    "\n",
    "no_lemma_train_test.to_csv('data/preprocessing/no_lemmatization_data_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6005d",
   "metadata": {},
   "source": [
    "## Word2vec dengan 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "04629a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# tokenisasi seluruh kata yang ada dalam corpus yang tanpa lemmatization\n",
    "no_lemma_corpus = [word_tokenize(review) for review in no_lemma_corpus_data.review]\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 100 dan window 5\n",
    "no_lemma100 = Word2Vec(\n",
    "            no_lemma_corpus,\n",
    "            vector_size = 100, # vektor size yang digunakan = 100\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "# simpan model yang telah dibangun\n",
    "no_lemma100.save(\"model/no_lemmatization/word2vec_100.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7bc9dc5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2309018 , -0.00153574,  0.14577068,  0.08995903,  0.05500919,\n",
       "       -0.665077  ,  0.23647559,  0.41211092,  0.12710327, -0.21367459,\n",
       "       -0.00314389, -0.3760469 ,  0.03656601,  0.3311016 , -0.12166534,\n",
       "       -0.02505325,  0.13970053, -0.21758987,  0.25795498, -0.41923523,\n",
       "        0.4130343 , -0.01939006,  0.31478587, -0.33007374,  0.32881382,\n",
       "        0.04090679, -0.15423372, -0.23840465, -0.36598474, -0.06728566,\n",
       "        0.04654999, -0.0278208 ,  0.28533974, -0.43626752, -0.2354558 ,\n",
       "        0.22966565,  0.3566799 ,  0.10919709,  0.04941875, -0.2554531 ,\n",
       "        0.03806031, -0.24625973, -0.42672178,  0.02701847,  0.21295863,\n",
       "        0.14888926, -0.09281895,  0.09722883, -0.04121841,  0.18143584,\n",
       "        0.06267954, -0.04359243, -0.02196747,  0.12676364, -0.14396071,\n",
       "       -0.17495137, -0.15417542,  0.24536097, -0.5498472 ,  0.30193865,\n",
       "        0.09906045,  0.05623512,  0.07147014, -0.20362289, -0.09462259,\n",
       "        0.07465757, -0.05354272,  0.19223586, -0.24891713,  0.5010239 ,\n",
       "        0.11867478,  0.2207562 ,  0.31409517, -0.00206886,  0.21841912,\n",
       "        0.01986717,  0.08742368, -0.02174289, -0.18459448,  0.1274687 ,\n",
       "       -0.2024129 , -0.07639654, -0.17361723,  0.2726815 , -0.16400637,\n",
       "       -0.16841523,  0.46419203,  0.36172193,  0.09992422,  0.4723983 ,\n",
       "        0.1599853 ,  0.19011033,  0.15578978,  0.06887715,  0.4993727 ,\n",
       "        0.37262675, -0.20356677, -0.21500319,  0.07625045,  0.28967077],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "no_lemma100.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81deb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model yang berisi word vector dari model yang dibangun\n",
    "no_lemma100 = Word2Vec.load(\"model/no_lemmatization/word2vec_100.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3cbf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs100 = [sent_vector(sentence, no_lemma100) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_features_vect100 = np.array(no_lemma_vecs100)\n",
    "no_lemma_data_features100 = pd.DataFrame(no_lemma_features_vect100)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "no_lemma_data_features100.to_csv('data/features/nolemmatization_features_100d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1842d",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7119b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  79.3\n",
      "Recall score =  79.29\n",
      "Precission score =  79.31\n",
      "F1-Score =  79.25999999999999\n",
      "Wall time: 452 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "nl_acc_score = cross_val_score(gnb_model, no_lemma_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nl_avg_accuracy = round((nl_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "nl_recall_score = cross_val_score(gnb_model, no_lemma_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nl_avg_recall = round((nl_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "nl_precission_score = cross_val_score(gnb_model, no_lemma_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nl_avg_precission = round((nl_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "nl_f_score = cross_val_score(gnb_model, no_lemma_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nl_avg_fscore = round((nl_f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", nl_avg_accuracy*100)\n",
    "print(\"Recall score = \", nl_avg_recall*100)\n",
    "print(\"Precission score = \", nl_avg_precission*100)\n",
    "print(\"F1-Score = \", nl_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fd43a",
   "metadata": {},
   "source": [
    "## Word2vec dengan 200 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3b4664f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 200 dan window 5\n",
    "no_lemma200 = Word2Vec(\n",
    "            no_lemma_corpus,\n",
    "            vector_size = 200, # vektor size yang digunakan = 200\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ") \n",
    "\n",
    "#simpan model yang dibangun\n",
    "no_lemma200.save(\"model/no_lemmatization/word2vec_200.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1ad585d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.47039998e-02,  9.53572243e-03, -1.41683161e-01,  1.90091968e-01,\n",
       "       -1.51713379e-02, -4.56017293e-02,  2.53462076e-01,  4.45601225e-01,\n",
       "       -5.70640042e-02,  8.46770182e-02, -1.91846281e-01, -1.74805343e-01,\n",
       "        9.94903371e-02,  1.85157210e-01, -2.32787907e-01, -1.42708614e-01,\n",
       "       -1.71228331e-02,  9.91923660e-02,  1.10119343e-01, -2.42103025e-01,\n",
       "        1.29805654e-01,  9.45796743e-02, -2.15318846e-03,  2.10435376e-01,\n",
       "        8.28198865e-02, -1.34406343e-01, -2.29942389e-02, -3.14852633e-02,\n",
       "       -9.21615139e-02,  3.96875478e-02,  1.29558161e-01,  8.05696473e-02,\n",
       "        1.86742004e-02, -4.60804850e-02,  4.08299863e-02,  5.87361120e-02,\n",
       "        2.29264125e-01, -1.14779204e-01,  1.90573260e-01, -4.51010466e-01,\n",
       "       -6.91183656e-02,  1.26331300e-01, -1.77019551e-01,  2.83963561e-01,\n",
       "        4.68605906e-01,  1.63209751e-01, -1.77081823e-01,  1.51969166e-02,\n",
       "        2.10442156e-01,  1.80309281e-01, -6.07352406e-02,  1.59769177e-01,\n",
       "       -1.57175541e-01, -1.19371742e-01,  4.75545824e-02, -1.90024555e-01,\n",
       "        1.05131967e-02,  7.25181401e-03, -4.45269257e-01,  2.85421491e-01,\n",
       "        2.39827111e-02,  1.16016448e-01,  1.51940640e-02, -5.13334684e-02,\n",
       "       -1.90481484e-01,  1.00660712e-01, -1.89689219e-01,  2.58619010e-01,\n",
       "       -3.25775385e-01,  6.82995170e-02, -7.88057446e-02,  2.23292530e-01,\n",
       "       -4.96714674e-02, -1.93531379e-01,  1.10520966e-01, -6.36154339e-02,\n",
       "        3.24408889e-01, -1.58293083e-01, -1.77666292e-01,  7.64666498e-02,\n",
       "        9.73225571e-03, -1.35488734e-01, -1.16875321e-01,  2.91397721e-02,\n",
       "       -1.61137238e-01, -2.24198937e-01,  2.35195607e-01,  1.55814543e-01,\n",
       "        1.70887455e-01,  1.63009807e-01, -2.84609362e-03,  1.63107827e-01,\n",
       "        3.16533834e-01,  1.04767874e-01,  3.40557992e-01, -1.62204616e-02,\n",
       "       -3.88915800e-02, -2.36000061e-01,  1.32958993e-01,  2.77155757e-01,\n",
       "       -4.88763869e-01,  3.24347496e-01,  1.94818109e-01, -1.05161062e-02,\n",
       "        1.14900254e-01, -2.10218012e-01,  8.47259313e-02,  1.00777403e-01,\n",
       "       -1.82638869e-01, -1.13608919e-01, -4.78585511e-02, -2.27039009e-01,\n",
       "        1.94431037e-01, -2.24752846e-04,  3.61381285e-02,  1.92024987e-02,\n",
       "        1.81781471e-01, -2.72616327e-01, -1.44934371e-01, -4.66045439e-01,\n",
       "       -5.69511391e-02,  2.08837658e-01,  2.23302409e-01, -1.53709695e-01,\n",
       "        6.66610822e-02,  3.25166397e-02, -1.30950212e-01,  1.70871586e-01,\n",
       "        8.44099149e-02,  1.46280363e-01,  1.32188305e-01,  6.63430104e-03,\n",
       "        1.62026733e-02, -2.45667338e-01, -9.61465854e-03,  3.50661397e-01,\n",
       "       -2.03299686e-01, -1.29168212e-01, -2.11620480e-02, -8.18044841e-02,\n",
       "        2.33566731e-01, -3.18546504e-01,  3.59407696e-03, -7.78735802e-03,\n",
       "       -1.90604717e-01, -2.30664052e-02, -1.67639881e-01,  2.47750953e-02,\n",
       "        7.65838176e-02,  9.15600955e-02,  3.16588551e-01, -1.11137748e-01,\n",
       "        5.83981350e-02,  4.06108126e-02, -4.02820818e-02, -5.53572401e-02,\n",
       "        2.43457943e-01,  2.41721228e-01, -4.96132746e-02, -4.64795977e-02,\n",
       "        5.82036339e-02,  2.21153587e-01, -3.94730084e-02,  2.15925068e-01,\n",
       "        5.09069040e-02,  1.26371786e-01,  1.03579938e-01, -3.16289335e-01,\n",
       "       -2.79666573e-01,  8.91558453e-02, -2.47713178e-01,  3.92333977e-02,\n",
       "        1.58234179e-01, -1.85617372e-01, -3.81878670e-03,  7.12468997e-02,\n",
       "       -3.07980239e-01,  3.30970287e-01,  3.22435439e-01,  1.55732110e-01,\n",
       "       -2.00490445e-01,  1.34190172e-01,  5.95292971e-02,  3.61376517e-02,\n",
       "        3.32196020e-02, -3.03509124e-02,  2.05732044e-02, -1.55442268e-01,\n",
       "        3.24472785e-01,  1.85977984e-02, -1.43107139e-02, -1.15625337e-01,\n",
       "       -6.80983663e-02,  4.63173278e-02,  1.53879032e-01, -4.05294336e-02,\n",
       "        3.69439036e-01, -1.64268672e-01, -7.43933544e-02,  7.88031071e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "no_lemma200.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c3e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model yang telah dibangun\n",
    "no_lemma200 = Word2Vec.load(\"model/no_lemmatization/word2vec_200.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652a6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs200 = [sent_vector(sentence, no_lemma200) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_features_vect200 = np.array(no_lemma_vecs200)\n",
    "no_lemma_data_features200 = pd.DataFrame(no_lemma_features_vect200)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "no_lemma_data_features200.to_csv('data/features/nolemmatization_features_200d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82d1e3",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300dcca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  79.58\n",
      "Recall score =  79.56\n",
      "Precission score =  79.60000000000001\n",
      "F1-Score =  79.53\n",
      "Wall time: 651 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "nl_acc_score = cross_val_score(gnb_model, no_lemma_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nl_avg_accuracy = round((nl_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "nl_recall_score = cross_val_score(gnb_model, no_lemma_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nl_avg_recall = round((nl_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "nl_precission_score = cross_val_score(gnb_model, no_lemma_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nl_avg_precission = round((nl_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "nl_f_score = cross_val_score(gnb_model, no_lemma_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nl_avg_fscore = round((nl_f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", nl_avg_accuracy*100)\n",
    "print(\"Recall score = \", nl_avg_recall*100)\n",
    "print(\"Precission score = \", nl_avg_precission*100)\n",
    "print(\"F1-Score = \", nl_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194a0c2",
   "metadata": {},
   "source": [
    "## Word2vec dengan 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "87c1c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 300 dan window 5\n",
    "no_lemma300 = Word2Vec(\n",
    "            no_lemma_corpus,\n",
    "            vector_size = 300, # vektor size yang digunakan = 300\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ") \n",
    "\n",
    "#simpan model yang telah dibangun\n",
    "no_lemma300.save(\"model/no_lemmatization/word2vec_300.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f8f37f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09275396,  0.15944067, -0.00754234,  0.07719963, -0.13330416,\n",
       "       -0.09093308,  0.25804374,  0.29238093,  0.04640691, -0.35263819,\n",
       "        0.13834521, -0.02910202,  0.11537385,  0.1130034 , -0.10332471,\n",
       "       -0.26269636, -0.02205137, -0.01784052,  0.09568539, -0.17817202,\n",
       "       -0.11573377, -0.11528245,  0.25688282,  0.05292636,  0.22485921,\n",
       "       -0.14374621, -0.18053098,  0.1505526 , -0.0827397 , -0.18648987,\n",
       "       -0.17580402, -0.13088652,  0.03383539, -0.09587415, -0.08457573,\n",
       "        0.05753246,  0.12810686, -0.21431173, -0.10846159, -0.14692956,\n",
       "       -0.08788489,  0.08532303, -0.02548132, -0.0923062 ,  0.02709454,\n",
       "        0.22885065, -0.1082556 ,  0.1643049 ,  0.05651328,  0.10526485,\n",
       "       -0.18479936,  0.17305683, -0.00568769,  0.08746602, -0.20989148,\n",
       "        0.11726239,  0.13152075,  0.20536168,  0.052592  ,  0.11677165,\n",
       "       -0.07089716, -0.22315903,  0.09263425,  0.19627094, -0.02205039,\n",
       "        0.11741387, -0.02129949,  0.04225313, -0.1625026 ,  0.0186984 ,\n",
       "        0.14343633,  0.17448606,  0.4177523 , -0.03141354, -0.1239774 ,\n",
       "        0.01137066,  0.00577398, -0.03043425,  0.04086514,  0.1589761 ,\n",
       "       -0.11210985, -0.12284436,  0.08655167,  0.4321763 , -0.07025807,\n",
       "       -0.02080898, -0.02243771,  0.04515481,  0.24272864, -0.00403495,\n",
       "        0.03433868, -0.1827303 ,  0.11518502,  0.19826807,  0.20724854,\n",
       "        0.01615038,  0.29356965, -0.15008934, -0.14739753,  0.08014974,\n",
       "       -0.08367905,  0.08563069,  0.11496363,  0.22492468,  0.05328162,\n",
       "       -0.1655216 , -0.11778302,  0.27807   , -0.20686981,  0.04208191,\n",
       "       -0.1623834 , -0.01661664, -0.01466094,  0.23401825,  0.16156146,\n",
       "        0.00169598,  0.06803272,  0.10424783,  0.19243932, -0.20088299,\n",
       "        0.16116157, -0.06775127, -0.00264393,  0.05547974, -0.01043797,\n",
       "        0.07817989, -0.07320146, -0.13008091,  0.11361046,  0.23384771,\n",
       "        0.08482851,  0.23451228,  0.22880399, -0.45145294,  0.12709217,\n",
       "       -0.00411299, -0.03633609,  0.10415683, -0.06968643, -0.10249828,\n",
       "        0.02104624, -0.0999885 , -0.14993341,  0.00716052,  0.08098724,\n",
       "       -0.00744562, -0.05571005, -0.12466902,  0.15978742, -0.08798954,\n",
       "        0.18957144, -0.19525692, -0.02447173,  0.11869495,  0.30515206,\n",
       "        0.23616907, -0.10496891,  0.06171473, -0.07337842,  0.1551954 ,\n",
       "        0.08215669, -0.11458208, -0.05936135,  0.04024275,  0.01256374,\n",
       "       -0.00336803,  0.06032448, -0.12739131,  0.18216231,  0.3248639 ,\n",
       "       -0.04164325,  0.10367278,  0.19227114, -0.05572393,  0.04854568,\n",
       "       -0.07784842, -0.18208794, -0.14996432, -0.02360705,  0.07339957,\n",
       "       -0.2612709 ,  0.2792063 , -0.234315  , -0.09288608, -0.18203603,\n",
       "       -0.1514236 ,  0.24022564,  0.2384898 ,  0.26546818, -0.23010403,\n",
       "        0.10612063, -0.05765477, -0.23581994, -0.06623956,  0.01072309,\n",
       "       -0.22598505,  0.1065366 , -0.15084942, -0.11653579,  0.11489893,\n",
       "       -0.15424848,  0.07518047, -0.01937779, -0.0797388 , -0.08108439,\n",
       "        0.20906724, -0.08263844,  0.22106421, -0.01056247,  0.02180014,\n",
       "       -0.24321467, -0.00376237,  0.08597586, -0.00211505,  0.33866638,\n",
       "       -0.0773036 ,  0.06421369, -0.26340854, -0.08705055, -0.37480024,\n",
       "       -0.06964107, -0.0603779 , -0.10097678, -0.13587824, -0.12638636,\n",
       "       -0.00364357,  0.01988868, -0.00579122,  0.10965883,  0.01512204,\n",
       "        0.07847107, -0.03662158, -0.22331683,  0.06329713, -0.11454526,\n",
       "        0.05777854,  0.05703034,  0.11629586,  0.20350544, -0.33042878,\n",
       "        0.04061049, -0.35555348, -0.16962396, -0.06232546,  0.11623827,\n",
       "       -0.06615461,  0.04751905, -0.0690019 , -0.04086177,  0.08158445,\n",
       "        0.02078928, -0.15333126, -0.06801125, -0.09882408, -0.2982345 ,\n",
       "       -0.04813921,  0.24153534,  0.16058645, -0.17946064, -0.1410114 ,\n",
       "        0.22138298, -0.08122108,  0.06079862, -0.12968925, -0.07879257,\n",
       "        0.121305  , -0.02841572,  0.11246987, -0.13218567, -0.0773374 ,\n",
       "       -0.14091522,  0.15474072, -0.09073376,  0.12541783,  0.18920097,\n",
       "        0.08371134,  0.03818495,  0.02608308, -0.09884425,  0.15980056,\n",
       "        0.21591857,  0.03485399, -0.11272319,  0.08253993, -0.07433744,\n",
       "       -0.10973424, -0.04850806,  0.00765594, -0.24738044,  0.20816866,\n",
       "        0.04964559,  0.22299406,  0.05156815, -0.0311875 ,  0.21082968,\n",
       "        0.2420472 , -0.03850267, -0.07804986,  0.1325337 , -0.15282272],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coba model word2vec yang dibangun dengan melihat vector dari kata movie\n",
    "no_lemma300.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61c214db",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lemma300 = Word2Vec.load(\"model/no_lemmatization/word2vec_300.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1251923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs300 = [sent_vector(sentence, no_lemma300) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_features_vect300 = np.array(no_lemma_vecs300)\n",
    "no_lemma_data_features300 = pd.DataFrame(no_lemma_features_vect300)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "no_lemma_data_features300.to_csv('data/features/nolemmatization_features_300d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02ccdb",
   "metadata": {},
   "source": [
    "### Naive Bayes Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a1bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  79.66\n",
      "Recall score =  79.65\n",
      "Precission score =  79.67\n",
      "F1-Score =  79.62\n",
      "Wall time: 855 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 5\n",
    "nl_acc_score = cross_val_score(gnb_model, no_lemma_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nl_avg_accuracy = round((nl_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 5\n",
    "nl_recall_score = cross_val_score(gnb_model, no_lemma_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nl_avg_recall = round((nl_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 5\n",
    "nl_precission_score = cross_val_score(gnb_model, no_lemma_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nl_avg_precission = round((nl_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 5\n",
    "nl_f_score = cross_val_score(gnb_model, no_lemma_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nl_avg_fscore = round((nl_f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", nl_avg_accuracy*100)\n",
    "print(\"Recall score = \", nl_avg_recall*100)\n",
    "print(\"Precission score = \", nl_avg_precission*100)\n",
    "print(\"F1-Score = \", nl_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7941c4",
   "metadata": {},
   "source": [
    "# Extraksi word2vec dengan Wikipedia Corpus\n",
    "\n",
    "# menggunakan data lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf70c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# ubah data agar berbentuk per kalimat\n",
    "wiki_corpus = word2vec.LineSentence('enwiki_dataset.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77443732",
   "metadata": {},
   "source": [
    "## Word2vec dengan 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a93db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 100 dan window 5\n",
    "wiki_100 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 100, # vektor size yang digunakan = 100\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "#simpan model\n",
    "wiki_100.save(\"model/wiki_model/wiki_model100.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d0184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_100 = Word2Vec.load(\"model/wiki_model/wiki_model100.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dcb9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "wiki_vecs100 = [sent_vector(sentence, wiki_100) for sentence in train_test.review]\n",
    "wiki_features_vec100 = np.array(wiki_vecs100)\n",
    "wiki_data_features100 = pd.DataFrame(wiki_features_vec100)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "wiki_data_features100.to_csv('data/features/wiki_data_features100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7ca24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  69.86\n",
      "Recall score =  69.78999999999999\n",
      "Precission score =  70.07\n",
      "F1-Score =  69.67999999999999\n",
      "Wall time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "wiki_acc_score = cross_val_score(gnb_model, wiki_data_features100, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "wiki_avg_accuracy = round((wiki_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "wiki_recall_score = cross_val_score(gnb_model, wiki_data_features100, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "wiki_avg_recall = round((wiki_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "wiki_precission_score = cross_val_score(gnb_model, wiki_data_features100, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "wiki_avg_precission = round((wiki_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "wiki_f_score = cross_val_score(gnb_model, wiki_data_features100, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "wiki_avg_fscore = round((wiki_f_score.mean()),4) # rata rata f1-score dengan menggunakan 10 fold cross validation\n",
    "\n",
    "print(\"Accuracy score = \", wiki_avg_accuracy*100)\n",
    "print(\"Recall score = \", wiki_avg_recall*100)\n",
    "print(\"Precission score = \", wiki_avg_precission*100)\n",
    "print(\"F1-Score = \", wiki_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a59ad5",
   "metadata": {},
   "source": [
    "### Word2vec dengan 200 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787dec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 200 dan window 5\n",
    "wiki_200 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 200, # vektor size yang digunakan = 200\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "#simpan model\n",
    "wiki_200.save(\"model/wiki_model/wiki_model200.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc377ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_200 = Word2Vec.load(\"model/wiki_model/wiki_model200.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c0990a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "wiki_vecs200 = [sent_vector(sentence, wiki_200) for sentence in train_test.review]\n",
    "wiki_features_vec200 = np.array(wiki_vecs200)\n",
    "wiki_data_features200 = pd.DataFrame(wiki_features_vec200)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "wiki_data_features200.to_csv('data/features/wiki_data_features200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3c563fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  70.39999999999999\n",
      "Recall score =  70.33\n",
      "Precission score =  70.56\n",
      "F1-Score =  70.25\n",
      "Wall time: 654 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "wiki_acc_score = cross_val_score(gnb_model, wiki_data_features200, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "wiki_avg_accuracy = round((wiki_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "wiki_recall_score = cross_val_score(gnb_model, wiki_data_features200, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "wiki_avg_recall = round((wiki_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "wiki_precission_score = cross_val_score(gnb_model, wiki_data_features200, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "wiki_avg_precission = round((wiki_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "wiki_f_score = cross_val_score(gnb_model, wiki_data_features200, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "wiki_avg_fscore = round((wiki_f_score.mean()),4)# rata rata f1-score dengan menggunakan 10 fold cross validation\n",
    "\n",
    "print(\"Accuracy score = \", wiki_avg_accuracy*100)\n",
    "print(\"Recall score = \", wiki_avg_recall*100)\n",
    "print(\"Precission score = \", wiki_avg_precission*100)\n",
    "print(\"F1-Score = \", wiki_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347674f",
   "metadata": {},
   "source": [
    "### Word2vec dengan 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "027655e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 300 dan window 5\n",
    "wiki_300 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 300, # vektor size yang digunakan = 300\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "#simpan model\n",
    "wiki_300.save(\"model/wiki_model/wiki_model300.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54bd9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_300 = Word2Vec.load(\"model/wiki_model/wiki_model300.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3492eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "wiki_vecs300 = [sent_vector(sentence, wiki_300) for sentence in train_test.review]\n",
    "wiki_features_vec300 = np.array(wiki_vecs300)\n",
    "wiki_data_features300 = pd.DataFrame(wiki_features_vec300)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "wiki_data_features300.to_csv('data/features/wiki_data_features300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "972ee83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  70.54\n",
      "Recall score =  70.46\n",
      "Precission score =  70.73\n",
      "F1-Score =  70.37\n",
      "Wall time: 897 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "wiki_acc_score = cross_val_score(gnb_model, wiki_data_features300, train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "wiki_avg_accuracy = round((wiki_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "wiki_recall_score = cross_val_score(gnb_model, wiki_data_features300, train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "wiki_avg_recall = round((wiki_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "wiki_precission_score = cross_val_score(gnb_model, wiki_data_features300, train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "wiki_avg_precission = round((wiki_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "wiki_f_score = cross_val_score(gnb_model, wiki_data_features300, train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "wiki_avg_fscore = round((wiki_f_score.mean()),4) # rata rata f1-score dengan menggunakan 10 fold cross validation\n",
    "\n",
    "print(\"Accuracy score = \", wiki_avg_accuracy*100)\n",
    "print(\"Recall score = \", wiki_avg_recall*100)\n",
    "print(\"Precission score = \", wiki_avg_precission*100)\n",
    "print(\"F1-Score = \", wiki_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a58054",
   "metadata": {},
   "source": [
    "# Extraksi word2vec dengan Wikipedia Corpus\n",
    "\n",
    "# menggunakan data tanpa lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29354691",
   "metadata": {},
   "source": [
    "## Word2vec dengan 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6245de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "no_lemma_wiki_100 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 100, # size of vector\n",
    "            window = 5, # context window size\n",
    "            min_count = 2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            workers = 32, # no.of cores\n",
    "            sg = 1 #inisate skip-gram model\n",
    ")\n",
    "\n",
    "#save model\n",
    "no_lemma_wiki_100.save(\"model/wiki_model/no_lemma_wiki_100.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d725b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lemma_wiki_100 = Word2Vec.load(\"model/wiki_model/no_lemma_wiki_100.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0f2e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs100 = [sent_vector(sentence, no_lemma_wiki_100) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_wiki_features_vec100 = np.array(no_lemma_vecs100)\n",
    "no_lemma_wiki_data_features100 = pd.DataFrame(no_lemma_wiki_features_vec100)\n",
    "\n",
    "# save train word vector to csv\n",
    "no_lemma_wiki_data_features100.to_csv('data/features/no_lemma_wiki_data_features100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3b60d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  70.42\n",
      "Recall score =  70.38\n",
      "Precission score =  70.61\n",
      "F1-Score =  70.27\n",
      "Wall time: 432 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 5\n",
    "nlw_acc_score = cross_val_score(gnb_model, no_lemma_wiki_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nlw_avg_accuracy = round((nlw_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 5 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 5\n",
    "nlw_recall_score = cross_val_score(gnb_model, no_lemma_wiki_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nlw_avg_recall = round((nlw_recall_score.mean()),4) # rata rata recall dengan menggunakan 5 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 5\n",
    "nlw_precission_score = cross_val_score(gnb_model, no_lemma_wiki_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nlw_avg_precission = round((nlw_precission_score.mean()),4) # rata rata precission dengan menggunakan 5 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 5\n",
    "nlw_f_score = cross_val_score(gnb_model, no_lemma_wiki_data_features100, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nlw_avg_fscore = round((nlw_f_score.mean()),4)\n",
    "\n",
    "print(\"Accuracy score = \", nlw_avg_accuracy*100)\n",
    "print(\"Recall score = \", nlw_avg_recall*100)\n",
    "print(\"Precission score = \", nlw_avg_precission*100)\n",
    "print(\"F1-Score = \", nlw_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed66f8",
   "metadata": {},
   "source": [
    "## Word2vec dengan 200 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "404af961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 200 dan window 5\n",
    "no_lemma_wiki_200 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 200, # vektor size yang digunakan = 200\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "#simpan model\n",
    "no_lemma_wiki_200.save(\"model/wiki_model/no_lemma_wiki_200.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b52d338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lemma_wiki_200 = Word2Vec.load(\"model/wiki_model/no_lemma_wiki_200.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db9a7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs200 = [sent_vector(sentence, no_lemma_wiki_200) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_wiki_features_vec200 = np.array(no_lemma_vecs200)\n",
    "no_lemma_wiki_data_features200 = pd.DataFrame(no_lemma_wiki_features_vec200)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "no_lemma_wiki_data_features200.to_csv('data/features/no_lemma_wiki_data_features200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f63bb6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  70.58\n",
      "Recall score =  70.54\n",
      "Precission score =  70.78\n",
      "F1-Score =  70.43\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "nlw_acc_score = cross_val_score(gnb_model, no_lemma_wiki_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nlw_avg_accuracy = round((nlw_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "nlw_recall_score = cross_val_score(gnb_model, no_lemma_wiki_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nlw_avg_recall = round((nlw_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "nlw_precission_score = cross_val_score(gnb_model, no_lemma_wiki_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nlw_avg_precission = round((nlw_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "nlw_f_score = cross_val_score(gnb_model, no_lemma_wiki_data_features200, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nlw_avg_fscore = round((nlw_f_score.mean()),4) # rata rata f1-score dengan menggunakan 10 fold cross validation\n",
    "\n",
    "print(\"Accuracy score = \", nlw_avg_accuracy*100)\n",
    "print(\"Recall score = \", nlw_avg_recall*100)\n",
    "print(\"Precission score = \", nlw_avg_precission*100)\n",
    "print(\"F1-Score = \", nlw_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1e464",
   "metadata": {},
   "source": [
    "## Word2vec 300 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78486925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bangun model skip-gram dengan vektor size 300 dan window 5\n",
    "no_lemma_wiki_300 = Word2Vec(\n",
    "            wiki_corpus,\n",
    "            vector_size = 300, # vektor size yang digunakan = 300\n",
    "            window = 5, # window size yang digunakan = 5\n",
    "            min_count = 2, # minimum kemunculan kata pada corpus                                  \n",
    "            workers = 32, # jumlah dari thread yang digunakan dalam pemrosesan\n",
    "            sg = 1 # inisialisasi penggunaan model skip-gram\n",
    ")\n",
    "\n",
    "#simpan model\n",
    "no_lemma_wiki_300.save(\"model/wiki_model/no_lemma_wiki_300.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02f4d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lemma_wiki_300 = Word2Vec.load(\"model/wiki_model/no_lemma_wiki_300.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e431b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah seluruh review pada data yang akan digunakan untuk membangun model\n",
    "no_lemma_vecs300 = [sent_vector(sentence, no_lemma_wiki_300) for sentence in no_lemma_train_test.review]\n",
    "no_lemma_wiki_features_vec300 = np.array(no_lemma_vecs300)\n",
    "no_lemma_wiki_data_features300 = pd.DataFrame(no_lemma_wiki_features_vec300)\n",
    "\n",
    "# simpan vektor data kedalam csv\n",
    "no_lemma_wiki_data_features300.to_csv('data/features/no_lemma_wiki_data_features300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f538cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian NB\n",
      "Accuracy score =  70.72\n",
      "Recall score =  70.67\n",
      "Precission score =  70.93\n",
      "F1-Score =  70.56\n",
      "Wall time: 836 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training Gaussian NB\")\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# definisikan k-fold yang akan digunakan\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "# akurasi model dengan cross validation k = 10\n",
    "nlw_acc_score = cross_val_score(gnb_model, no_lemma_wiki_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'accuracy', n_jobs=-1)\n",
    "nlw_avg_accuracy = round((nlw_acc_score.mean()),4) # rata rata akurasi dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# recall model dengan cross validation k = 10\n",
    "nlw_recall_score = cross_val_score(gnb_model, no_lemma_wiki_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'recall_macro')\n",
    "nlw_avg_recall = round((nlw_recall_score.mean()),4) # rata rata recall dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# precission model dengan cross validation k = 10\n",
    "nlw_precission_score = cross_val_score(gnb_model, no_lemma_wiki_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'precision_macro')\n",
    "nlw_avg_precission = round((nlw_precission_score.mean()),4) # rata rata precission dengan menggunakan 10 fold cross validation\n",
    "\n",
    "# f1-score model dengan cross validation k = 10\n",
    "nlw_f_score = cross_val_score(gnb_model, no_lemma_wiki_data_features300, no_lemma_train_test.sentiment, cv=cv, scoring = 'f1_macro')\n",
    "nlw_avg_fscore = round((nlw_f_score.mean()),4) # rata rata f1-score dengan menggunakan 10 fold cross validation\n",
    "\n",
    "print(\"Accuracy score = \", nlw_avg_accuracy*100)\n",
    "print(\"Recall score = \", nlw_avg_recall*100)\n",
    "print(\"Precission score = \", nlw_avg_precission*100)\n",
    "print(\"F1-Score = \", nlw_avg_fscore*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b108c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
